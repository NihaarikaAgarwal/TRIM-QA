{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7WCgrEJ2h5p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/itewari1/.local/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/itewari1/.local/lib/python3.12/site-packages (from sentence-transformers) (4.51.2)\n",
      "Requirement already satisfied: tqdm in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/itewari1/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /home/itewari1/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/itewari1/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/itewari1/.local/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/itewari1/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/itewari1/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/itewari1/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/itewari1/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/itewari1/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/itewari1/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/itewari1/.local/lib/python3.12/site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in /home/itewari1/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/itewari1/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/itewari1/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers \n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Xw-sWTvY2fKk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel, TapasTokenizer, TapasForSequenceClassification\n",
    "import jsonlines\n",
    "import time\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set tokenizers parallelism to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FaRU7TzaWvUT"
   },
   "outputs": [],
   "source": [
    "# Load SBERT model for scoring columns\n",
    "column_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # ~22MB, fast and good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hlp6FkYbajod"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained BERT for scoring rows\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "row_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "row_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a4d55727cb4d00a780f6ecbda74d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/490 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f18ca0af84d4b5a82dc7dfe30f04d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/262k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ed0e7593cd4ea692a44f8d3316f23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb15b2b677d4af880973dd60111071b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9da9e2c35d84a8db21393e610bf79d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe42e36e4d6041698a5375d3ae27d46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TapasForSequenceClassification(\n",
       "  (tapas): TapasModel(\n",
       "    (embeddings): TapasEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings_0): Embedding(3, 768)\n",
       "      (token_type_embeddings_1): Embedding(256, 768)\n",
       "      (token_type_embeddings_2): Embedding(256, 768)\n",
       "      (token_type_embeddings_3): Embedding(2, 768)\n",
       "      (token_type_embeddings_4): Embedding(256, 768)\n",
       "      (token_type_embeddings_5): Embedding(256, 768)\n",
       "      (token_type_embeddings_6): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.07, inplace=False)\n",
       "    )\n",
       "    (encoder): TapasEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): TapasPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.07, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load TAPAS in classification mode\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-tabfact\")\n",
    "model = TapasForSequenceClassification.from_pretrained(\"google/tapas-base-finetuned-tabfact\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BbwescLcUEqv"
   },
   "outputs": [],
   "source": [
    "def fetch_chunks(table_id):\n",
    "  # Read chunks from chunks.json that match the target table\n",
    "    print(\"Loading and filtering chunks for target table...\")\n",
    "    # chunks = []\n",
    "    row_chunks = []\n",
    "    column_chunks = []\n",
    "    chunk_count = 0\n",
    "    matched_chunks = 0\n",
    "\n",
    "    with jsonlines.open('chunks.json', 'r') as reader:\n",
    "        for chunk in reader:\n",
    "            chunk_count += 1\n",
    "\n",
    "            # Check if the chunk belongs to our target table or any of its alternative IDs\n",
    "            if 'metadata' in chunk and 'table_name' in chunk['metadata'] and chunk['metadata']['table_name'] in table_id:\n",
    "                # chunks.append(chunk)\n",
    "                if chunk['metadata']['chunk_type'] == 'row':\n",
    "                    row_chunks.append(chunk)\n",
    "                else:\n",
    "                    column_chunks.append(chunk)\n",
    "                matched_chunks += 1\n",
    "\n",
    "    print(f\"Found {matched_chunks} chunks that match table ID '{table_id}' out of {chunk_count} total chunks\")\n",
    "\n",
    "    return row_chunks, column_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9k0x335zVBnK"
   },
   "outputs": [],
   "source": [
    "# def fetch_column_headers(column_chunks):\n",
    "#     column_headers = []\n",
    "#     for chunk in column_chunks:\n",
    "#         # if chunk.get('metadata', {}).get('chunk_type') == 'column':\n",
    "#         metadata_text = chunk['metadata'].get('metadata_text', '')\n",
    "\n",
    "#         # Extract 'col: ...' using simple string split\n",
    "#         for part in metadata_text.split(','):\n",
    "#             part = part.strip()\n",
    "#             if part.startswith('col:'):\n",
    "#                 col_name = part.replace('col:', '').strip()\n",
    "#                 column_headers.append(col_name)\n",
    "#                 break  # once 'col:' is found, we can move to next chunk\n",
    "\n",
    "#     print(\"Extracted column names:\")\n",
    "#     print(column_headers)\n",
    "#     return column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_column_headers_with_values(column_chunks):\n",
    "#     \"\"\"\n",
    "#     Extracts column headers and a single value (first non-empty) per column chunk.\n",
    "\n",
    "#     Args:\n",
    "#         column_chunks (list): List of column chunks generated by `chunk_column`.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: {column_name: one_value_or_dummy}\n",
    "#     \"\"\"\n",
    "#     column_data = {}\n",
    "\n",
    "#     for chunk in column_chunks:\n",
    "#         metadata_text = chunk['metadata'].get('metadata_text', '')\n",
    "#         chunk_text = chunk.get('text', '')\n",
    "\n",
    "#         # Extract column name from metadata_text\n",
    "#         col_name = None\n",
    "#         for part in metadata_text.split(','):\n",
    "#             part = part.strip()\n",
    "#             if part.startswith('col:'):\n",
    "#                 col_name = part.replace('col:', '').strip()\n",
    "#                 break\n",
    "\n",
    "#         if col_name:\n",
    "#             # Default value\n",
    "#             value = \"dummy\"\n",
    "\n",
    "#             # Extract values after the colon\n",
    "#             if \":\" in chunk_text:\n",
    "#                 try:\n",
    "#                     _, value_part = chunk_text.split(\":\", 1)\n",
    "#                     values = [v.strip() for v in value_part.split(\"|\") if v.strip()]\n",
    "#                     if values:\n",
    "#                         value = values[0]  # Take the first non-empty value\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "\n",
    "#             column_data[col_name] = value\n",
    "\n",
    "#     print(\"Extracted column/value pairs:\")\n",
    "#     print(column_data)\n",
    "#     return column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "P0c90JIeVzm-"
   },
   "outputs": [],
   "source": [
    "# def score_column_headers_sbert(query, column_headers):\n",
    "#     \"\"\"\n",
    "#     Compute cosine similarity between the query and each column header using SBERT.\n",
    "\n",
    "#     Args:\n",
    "#         query (str): natural language query.\n",
    "#         column_headers (List[str]): list of column names to score.\n",
    "\n",
    "#     Returns:\n",
    "#         List of tuples: (column_name, similarity_score), sorted descending.\n",
    "#     \"\"\"\n",
    "#     # Encode query and headers\n",
    "#     query_emb = column_model.encode(query, convert_to_tensor=True)\n",
    "#     header_embs = column_model.encode(column_headers, convert_to_tensor=True)\n",
    "\n",
    "#     # Compute cosine similarity\n",
    "#     cosine_scores = util.cos_sim(query_emb, header_embs)[0]  # shape: [num_headers]\n",
    "\n",
    "#     # Zip and sort\n",
    "#     scored_headers = list(zip(column_headers, cosine_scores.tolist()))\n",
    "#     scored_headers.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     return scored_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_column_headers_with_tapas(query: str, column_data: dict) -> float:\n",
    "#     \"\"\"\n",
    "#     Scores each column individually for relevance to the query using TAPAS classification mode.\n",
    "\n",
    "#     Args:\n",
    "#         query (str): Natural language question.\n",
    "#         column_data (dict): {column_name: one_value_or_dummy}\n",
    "\n",
    "#     Returns:\n",
    "#         dict: {column_name: relevance_score (float)}\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "\n",
    "#     for col, value in column_data.items():\n",
    "#         df = pd.DataFrame([{col: value}])  # Create single-column, single-row DataFrame\n",
    "\n",
    "#         try:\n",
    "#             # Tokenize input\n",
    "#             inputs = tokenizer(table=df, queries=[query], return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "#             # Run model\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(**inputs)\n",
    "#                 logits = outputs.logits  # shape: [1, 2]\n",
    "#                 probs = torch.sigmoid(logits)\n",
    "#                 relevance_score = round(probs[0, 0].item(), 4)  # entailment class\n",
    "\n",
    "#             results.append((col, relevance_score))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error scoring column '{col}': {e}\")\n",
    "#             results.append((col, None))\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_column_chunks_with_sbert(query: str, column_chunks: List[dict]):\n",
    "    \"\"\"\n",
    "    Scores each column chunk against a query using SBERT.\n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language question.\n",
    "        column_chunks (list): List of column chunks (from chunk_column()).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: [(column_name, similarity_score)] sorted by score descending.\n",
    "    \"\"\"\n",
    "    scored_columns = []\n",
    "\n",
    "    # Precompute query embedding\n",
    "    query_embedding = column_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    for chunk in column_chunks:\n",
    "        metadata_text = chunk.get(\"metadata\", {}).get(\"metadata_text\", \"\")\n",
    "        chunk_text = chunk.get(\"text\", \"\")\n",
    "\n",
    "        # Extract column name\n",
    "        col_name = None\n",
    "        for part in metadata_text.split(','):\n",
    "            part = part.strip()\n",
    "            if part.startswith('col:'):\n",
    "                col_name = part.replace('col:', '').strip()\n",
    "                break\n",
    "\n",
    "        if not col_name:\n",
    "            continue\n",
    "\n",
    "        # Compute similarity between query and full column text (header + sample values)\n",
    "        column_embedding = column_model.encode(chunk_text, convert_to_tensor=True)\n",
    "        score = util.pytorch_cos_sim(query_embedding, column_embedding).item()\n",
    "        score = round((score + 1) / 2, 4)  # normalize cosine similarity to [0, 1]\n",
    "\n",
    "        scored_columns.append((col_name, score))\n",
    "\n",
    "    # Sort by score descending\n",
    "    scored_columns.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return scored_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qXYk97pfWnnG"
   },
   "outputs": [],
   "source": [
    "def filter_chunks_by_column_score(chunks, scored_columns, threshold):\n",
    "    \"\"\"\n",
    "    Filters column chunks from the list of chunks based on precomputed SBERT scores.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of all chunk dictionaries.\n",
    "        scored_columns (list of tuples): List of (column_name, score) tuples from SBERT.\n",
    "        threshold (float): Minimum score required to retain a chunk.\n",
    "\n",
    "    Returns:\n",
    "        filtered_chunks (list): Chunks whose column name score >= threshold.\n",
    "    \"\"\"\n",
    "    # Build a set of column names to keep\n",
    "    valid_columns = {col for col, score in scored_columns if score >= threshold}\n",
    "\n",
    "    # Filter chunks that are 'column' type and have a matching valid column name\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        metadata_text = chunk[\"metadata\"].get(\"metadata_text\", \"\")\n",
    "        for part in metadata_text.split(','):\n",
    "            part = part.strip()\n",
    "            if part.startswith(\"col:\"):\n",
    "                col_name = part.replace(\"col:\", \"\").strip()\n",
    "                if col_name in valid_columns:\n",
    "                    filtered_chunks.append(chunk)\n",
    "                break\n",
    "\n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "71R1OMC7a_zC"
   },
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def score_row_tabert(chunks: List[Dict], query: str) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Scores each row chunk based on its semantic similarity to the query using BERT (as a proxy for TaBERT).\n",
    "\n",
    "#     Args:\n",
    "#         chunks (List[Dict]): List of row chunks with 'text' and 'metadata' keys\n",
    "#         query (str): Natural language question\n",
    "\n",
    "#     Returns:\n",
    "#         List[Dict]: List of dicts with chunk_id and relevance score\n",
    "#     \"\"\"\n",
    "#     scores = []\n",
    "\n",
    "#     # Encode the query once\n",
    "#     query_input = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#     query_output = row_model(**query_input)\n",
    "#     query_embedding = query_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         chunk_text = chunk[\"text\"]\n",
    "#         chunk_id = chunk[\"metadata\"][\"chunk_id\"]\n",
    "\n",
    "#         # Encode the chunk\n",
    "#         chunk_input = tokenizer(chunk_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#         chunk_output = row_model(**chunk_input)\n",
    "#         chunk_embedding = chunk_output.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "#         # Compute cosine similarity\n",
    "#         sim = torch.nn.functional.cosine_similarity(query_embedding, chunk_embedding).item()\n",
    "#         sim_normalized = (sim + 1) / 2  # normalize to [0,1]\n",
    "\n",
    "#         scores.append({\n",
    "#             \"chunk_id\": chunk_id,\n",
    "#             \"score\": round(sim_normalized, 4),\n",
    "#         })\n",
    "\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_row_chunks_with_sbert(row_chunks: List[Dict], query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scores each row chunk using SBERT based on semantic similarity to the query.\n",
    "\n",
    "    Args:\n",
    "        row_chunks (List[Dict]): List of chunks in your custom format, with 'text' and 'metadata'\n",
    "        query (str): The input natural language query\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of dicts with chunk_id and similarity score (normalized 0–1)\n",
    "    \"\"\"\n",
    "    # Pre-encode the query\n",
    "    query_emb = column_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    scored_chunks = []\n",
    "    for chunk in row_chunks:\n",
    "        chunk_text = chunk.get(\"text\", \"\")\n",
    "        chunk_id = chunk.get(\"metadata\", {}).get(\"chunk_id\", \"\")\n",
    "\n",
    "        # Encode chunk text and compute cosine similarity\n",
    "        chunk_emb = column_model.encode(chunk_text, convert_to_tensor=True)\n",
    "        sim_score = util.pytorch_cos_sim(query_emb, chunk_emb).item()\n",
    "\n",
    "        # Normalize cosine similarity from [-1, 1] → [0, 1]\n",
    "        normalized_score = round((sim_score + 1) / 2, 4)\n",
    "\n",
    "        scored_chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"score\": normalized_score\n",
    "        })\n",
    "\n",
    "    return scored_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tbHi6m9Hfyiu"
   },
   "outputs": [],
   "source": [
    "def filter_chunks_by_row_score(chunks, scored_rows, threshold):\n",
    "    \"\"\"\n",
    "    Filters row chunks from the list of chunks based on precomputed semantic similarity scores.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of all chunk dictionaries (from chunk_row).\n",
    "        scored_rows (list of dict): List of {'chunk_id': str, 'score': float} dicts.\n",
    "        threshold (float): Minimum score required to retain a chunk.\n",
    "\n",
    "    Returns:\n",
    "        filtered_chunks (list): Chunks whose score >= threshold.\n",
    "    \"\"\"\n",
    "    # Build a set of valid chunk_ids to retain\n",
    "    valid_ids = {row['chunk_id'] for row in scored_rows if row['score'] >= threshold}\n",
    "\n",
    "    # Filter chunks based on matching chunk_id\n",
    "    filtered_chunks = [\n",
    "        chunk for chunk in chunks\n",
    "        if chunk['metadata']['chunk_id'] in valid_ids\n",
    "    ]\n",
    "\n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_alpha(scores, base=0.5, min_alpha=0.1, max_alpha=1.0):\n",
    "    std_dev = np.std(scores)\n",
    "    normalized_std = np.clip(std_dev / 0.2, 0, 1)  # 0.2 is a tunable baseline\n",
    "    alpha = min_alpha + (max_alpha - min_alpha) * normalized_std\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_threshold(scores, alpha):\n",
    "    \"\"\"\n",
    "    Calculate a dynamic threshold using mean + alpha * std deviation.\n",
    "\n",
    "    Args:\n",
    "        scores (list or np.array): List of similarity scores (floats between 0 and 1).\n",
    "        alpha (float): Multiplier for standard deviation (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "        float: Threshold value\n",
    "    \"\"\"\n",
    "    # alpha = adaptive_alpha(scores)\n",
    "    # scores = np.array(scores)\n",
    "    \n",
    "    # if len(scores) == 0:\n",
    "    #     raise ValueError(\"Score list is empty.\")\n",
    "    \n",
    "    # mean = np.mean(scores)\n",
    "    # std_dev = np.std(scores)\n",
    "    # threshold = mean + alpha * std_dev\n",
    "\n",
    "    # return threshold\n",
    "    median = np.median(scores)\n",
    "    std_dev = np.std(scores)\n",
    "    return median + alpha * std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "J149T_wRhCeY"
   },
   "outputs": [],
   "source": [
    "def column_chunks_to_dataframe(column_chunks):\n",
    "    \"\"\"\n",
    "    Converts a list of column chunks into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        column_chunks (list): List of chunks, each containing a column in 'text' and metadata.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Structured DataFrame with headers and rows.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    for chunk in column_chunks:\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "\n",
    "        # Only process if the format is correct\n",
    "        if \":\" in text:\n",
    "            header, values_str = text.split(\":\", 1)\n",
    "            header = header.strip()\n",
    "\n",
    "            if not header:\n",
    "                continue\n",
    "\n",
    "            # Split values and remove empty strings\n",
    "            values = [v.strip() for v in values_str.strip().split(\"|\") if v.strip()]\n",
    "            data[header] = values\n",
    "\n",
    "    # Normalize column lengths by padding with empty strings\n",
    "    max_len = max((len(vals) for vals in data.values()), default=0)\n",
    "    for header, values in data.items():\n",
    "        if len(values) < max_len:\n",
    "            values.extend([\"\"] * (max_len - len(values)))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l3LLgCS1hHu0"
   },
   "outputs": [],
   "source": [
    "def row_chunks_to_dataframe(row_chunks):\n",
    "    \"\"\"\n",
    "    Converts row-based chunks with inline 'Header: Value' format into a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row_chunks (list): List of row-type chunk dicts.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Structured table with one row per chunk and appropriate columns.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    all_columns = set()\n",
    "\n",
    "    for chunk in row_chunks:\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "        column_names = chunk.get(\"metadata\", {}).get(\"columns\", [])\n",
    "        row_data = dict.fromkeys(column_names, \"\")  # initialize with empty strings\n",
    "\n",
    "        # Split on pipe and extract 'key: value' pairs\n",
    "        for part in text.split(\"|\"):\n",
    "            if \":\" in part:\n",
    "                key, value = part.split(\":\", 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                if key in column_names:\n",
    "                    row_data[key] = value\n",
    "\n",
    "        rows.append(row_data)\n",
    "        all_columns.update(column_names)\n",
    "\n",
    "    # Build DataFrame with all columns\n",
    "    df = pd.DataFrame(rows, columns=sorted(all_columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wzYHWrI4hN3x"
   },
   "outputs": [],
   "source": [
    "def intersect_row_and_column_dfs(df_row, df_col):\n",
    "    \"\"\"\n",
    "    Computes row-wise intersection between a row-based DataFrame and a column-based DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_row (pd.DataFrame): DataFrame constructed from row chunks.\n",
    "        df_col (pd.DataFrame): DataFrame constructed from column chunks.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Rows common to both DataFrames (intersection).\n",
    "    \"\"\"\n",
    "    # Align columns\n",
    "    common_cols = sorted(set(df_row.columns).intersection(set(df_col.columns)))\n",
    "    if not common_cols:\n",
    "        print(\"⚠️ No overlapping columns found between the DataFrames.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_row_sub = df_row[common_cols].copy()\n",
    "    df_col_sub = df_col[common_cols].copy()\n",
    "\n",
    "    # Drop NA to avoid mismatch due to missing values\n",
    "    df_row_sub = df_row_sub.dropna()\n",
    "    df_col_sub = df_col_sub.dropna()\n",
    "\n",
    "    # Deduplicate if necessary\n",
    "    df_row_sub = df_row_sub.drop_duplicates()\n",
    "    df_col_sub = df_col_sub.drop_duplicates()\n",
    "\n",
    "    # Perform intersection\n",
    "    intersected = pd.merge(df_row_sub, df_col_sub, how='inner')\n",
    "\n",
    "    return intersected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_json_entry(df, table_id):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame into a JSON-serializable dict matching the required format.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The pruned table.\n",
    "        table_id (str): Unique table identifier.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON-compatible dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    json_entry = {\n",
    "        \"id\": table_id,\n",
    "        \"table\": {\n",
    "            \"columns\": [{\"text\": str(col)} for col in df.columns],\n",
    "            \"rows\": [{\"cells\": [{\"text\": str(cell)} for cell in row]} for _, row in df.iterrows()],\n",
    "            \"tableId\": table_id,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return json_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_table(table_id, question):\n",
    "    ### Fetch chunks for the given table ID ###\n",
    "    row_chunks, column_chunks = fetch_chunks(table_id)\n",
    "    columns_df = column_chunks_to_dataframe(column_chunks)\n",
    "    row_df = row_chunks_to_dataframe(row_chunks)\n",
    "    # print(\"Column DF:\", columns_df)\n",
    "    # print(\"Row DF:\", row_df)\n",
    "\n",
    "    ### Column Level Pruning ###\n",
    "    # column_headers_with_values = fetch_column_headers_with_values(column_chunks)\n",
    "\n",
    "    # column_scores = score_column_headers_sbert(question, column_headers)\n",
    "    # column_scores = score_column_headers_with_tapas(question, column_headers_with_values)\n",
    "    column_scores = score_column_chunks_with_sbert(question, column_chunks)\n",
    "    # for col, score in column_scores:\n",
    "        # print(f\"  {col:15s} → {score:.4f}\")\n",
    "\n",
    "    # print(f\"Relevance score for table: {column_scores:.4f}\")\n",
    "    \n",
    "    column_scores_only = [score for _, score in column_scores]\n",
    "    alpha = -0.2 #check if this can be learned\n",
    "    column_threshold = dynamic_threshold(column_scores_only, alpha)\n",
    "    # print(\"Column Threshold:\", column_threshold)\n",
    "\n",
    "    filtered_column_chunks = filter_chunks_by_column_score(column_chunks, column_scores, column_threshold)\n",
    "    # print(filtered_column_chunks)\n",
    "\n",
    "    ### Row level Pruning ###\n",
    "    # row_scores = score_row_tabert(row_chunks, question)\n",
    "    row_scores = score_row_chunks_with_sbert(row_chunks, question)\n",
    "\n",
    "    # for item in row_scores:\n",
    "        # print(f\"{item['chunk_id']} → Score: {item['score']}\")\n",
    "\n",
    "    row_scores_only = [chunk[\"score\"] for chunk in row_scores]\n",
    "    alpha = 0.5 #check if this can be learned\n",
    "    row_threshold = dynamic_threshold(row_scores_only, alpha)\n",
    "    # print(\"Row Threshold:\", row_threshold)\n",
    "\n",
    "    filtered_row_chunks = filter_chunks_by_row_score(row_chunks, row_scores, row_threshold)\n",
    "\n",
    "    # print(f\"Computing similarity scores... done!\")\n",
    "\n",
    "    filtered_columns_df = column_chunks_to_dataframe(filtered_column_chunks)\n",
    "    filtered_row_df = row_chunks_to_dataframe(filtered_row_chunks)\n",
    "\n",
    "    # print(\"column DF\")\n",
    "    # print(filtered_columns_df)\n",
    "    # print('-' * 80)\n",
    "    # print(\"row DF\")\n",
    "    # print(filtered_row_df)\n",
    "\n",
    "    pruned_df = intersect_row_and_column_dfs(filtered_row_df, filtered_columns_df)\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(\"final DF\")\n",
    "    print(pruned_df)\n",
    "\n",
    "    return pruned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "whUFYkQvNZIF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the chief of the gods according to ancient greek myth\n",
      "Loading and filtering chunks for target table...\n",
      "Found 13 chunks that match table ID 'List of pharaohs_A3680D6D69C5E013' out of 2881668 total chunks\n",
      "⚠️ No overlapping columns found between the DataFrames.\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Loading and filtering chunks for target table...\n",
      "Found 54 chunks that match table ID 'List of historical period drama films and series set in Near Eastern and Western civilization_AA41E291AC14FC48' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                                    Notes on setting  \\\n",
      "0  Docudrama which depicts a re-enactment of the ...   \n",
      "1       Egypt, probably during the reign of Ahmose I   \n",
      "2  based on the novel by Mika Waltari, who adapte...   \n",
      "3  Egypt, during the reign of Ramesses II, based ...   \n",
      "4  Egypt, during the reign of Ramesses II, based ...   \n",
      "5  Greece immediately before the soldiers set sai...   \n",
      "6             western Anatolia during the Trojan War   \n",
      "7             western Anatolia during the Trojan War   \n",
      "8  the daughter of the Greek king Agamemnon seeks...   \n",
      "9  the ten-year struggle of Odysseus to return ho...   \n",
      "\n",
      "                                         Title  \n",
      "0  Atlantis: End of a World, Birth of a Legend  \n",
      "1                                       Joseph  \n",
      "2                                 The Egyptian  \n",
      "3                         The Ten Commandments  \n",
      "4                       Exodus: Gods and Kings  \n",
      "5                                    Iphigenia  \n",
      "6                                Helen of Troy  \n",
      "7                                         Troy  \n",
      "8                                      Electra  \n",
      "9                                      Ulysses  \n",
      "Loading and filtering chunks for target table...\n",
      "Found 26 chunks that match table ID 'List of medical roots, suffixes and prefixes_A2E6D18E2407DDF3' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                Affix              Example(s)  \\\n",
      "0             lei(o)-               Leiomyoma   \n",
      "1            lept(o)-          Leptomeningeal   \n",
      "2  leuc(o)-, leuk(o)-               Leukocyte   \n",
      "3            lith(o)-             Lithotripsy   \n",
      "4             log(o)-  dialog, catalog, logos   \n",
      "5              -lysis               Paralysis   \n",
      "\n",
      "                  Origin language and etymology  \n",
      "0                                   Greek λεῖος  \n",
      "1                         Greek λεπτός (leptos)  \n",
      "2  Ancient Greek λευκός (leukos), white, bright  \n",
      "3                          Greek λίθος (lithos)  \n",
      "4                           Greek λόγος (logos)  \n",
      "5                                   Greek λύσις  \n",
      "Loading and filtering chunks for target table...\n",
      "Found 107 chunks that match table ID 'Monarch_FEA5372160C0FCA6' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                                  Description and use  \\\n",
      "0    Title of the king of the Ashanti people in Ghana   \n",
      "1   Ruler of Tunisia until 1957; originally Turkis...   \n",
      "2                                  Leader of a people   \n",
      "3                                     Emperor of Mali   \n",
      "4   Emperor of Ethiopia; properly Negus Negust, me...   \n",
      "5                            Emperor of Ancient Egypt   \n",
      "6                            King of the Hausa people   \n",
      "7             \"Divine Ruler\"; ruled Sikkim until 1975   \n",
      "8   title of leaders of small principalities in An...   \n",
      "9   Also known as Huángdì, rule the Imperial China...   \n",
      "10  The title of the ruler of Baroda (India). The ...   \n",
      "11                  Limbu King of East Nepal Limbuwan   \n",
      "12     Ancient and modern Filipino equivalent of king   \n",
      "13       皇帝 as in Chinese, the Imperial China Emperor   \n",
      "14                    Title used in Aceh before Islam   \n",
      "15  King of Cambodia Khmer, the title literally me...   \n",
      "16  King of Thailand (Siam), the title literally m...   \n",
      "17          Title of the ruler of Gwalior (India)[11]   \n",
      "18  Pre-Imperial China/Russia. \"King\" is the usual...   \n",
      "19  The king of Korea that control over all of Kor...   \n",
      "20  Monarch of Malaysia who is elected every five ...   \n",
      "21  Nahuatl King. The word literally means \"speake...   \n",
      "22  also known as Apu (\"divinity\"), Inka Qhapaq (\"...   \n",
      "23                     \"King\" during Mycenaean Greece   \n",
      "24    Greek term for the Roman and Byzantine Emperors   \n",
      "25  \"King\" in ancient Greece, Thrace, Macedonia, C...   \n",
      "26  Byzantine Empire, Second Bulgarian Empire, Dan...   \n",
      "\n",
      "                                              Title  \n",
      "0                                        Asantehene  \n",
      "1                                               Bey  \n",
      "2                                         Chieftain  \n",
      "3                                             Mansa  \n",
      "4                                             Negus  \n",
      "5                                           Pharaoh  \n",
      "6                                             Sarki  \n",
      "7                                           Chogyal  \n",
      "8                                              Datu  \n",
      "9                                  Emperor of China  \n",
      "10                                          Gaekwad  \n",
      "11                                             Hang  \n",
      "12                                             Harì  \n",
      "13                                          Huángdì  \n",
      "14                                           Meurah  \n",
      "15  Preah Karuna Preah Bat Sâmdech Preah Bâromneath  \n",
      "16                     Phrabat Somdej Phrachaoyuhua  \n",
      "17                                          Scindia  \n",
      "18                                             Wang  \n",
      "19                                             Wang  \n",
      "20                            Yang di-Pertuan Agong  \n",
      "21                                         Tlatoani  \n",
      "22                                        Sapa Inca  \n",
      "23                                             Anax  \n",
      "24                                       Autokrator  \n",
      "25                                         Basileus  \n",
      "26                                           Despot  \n",
      "Loading and filtering chunks for target table...\n",
      "Found 33 chunks that match table ID 'Olympian Gods (DC Comics)_DE50A89336AE7CAE' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                                          Description      Member\n",
      "0   Apollo is the God of the Sun, Music, Poetry, O...      Apollo\n",
      "1   Ares is the God of War, Violence, Bloodlust, C...        Ares\n",
      "2   Athena is the Goddess of Wisdom, Strategy, Cra...      Athena\n",
      "3   Dionysus is the God of Wine, Partying, Festivi...    Dionysus\n",
      "4   Eros is the God of Desire, Lust, and Attractio...        Eros\n",
      "5   Hades is the God of the Underworld, Dead and W...       Hades\n",
      "6   Hephaestus is the God of Fire, Blacksmithing, ...  Hephaestus\n",
      "7   Hera is the Queen of the Gods, Goddess of Marr...        Hera\n",
      "8   Heracles, also known as Hercules, is the son o...    Heracles\n",
      "9   Hermes is the Messenger of the Gods, and the G...      Hermes\n",
      "10  Phobos is the God of Fear who is based on the ...      Phobos\n",
      "11  Poseidon is the God of the Sea, Earthquakes, a...    Poseidon\n",
      "12  Zeus is the ruler of Olympus, King of the Gods...        Zeus\n",
      "Loading and filtering chunks for target table...\n",
      "Found 15 chunks that match table ID 'Flags of Europe_B0E3EF092429C8AA' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                                         Description                     Use\n",
      "0  The flag of Spetses, adopted in early 1821 dur...         Flag of Spetses\n",
      "1  The flag of the island of Zakynthos, adopted i...       Flag of Zakynthos\n",
      "2  The flag of the island of Psara, adopted in 18...           Flag of Psara\n",
      "3  The flag of the island of the Mani Peninsula, ...  Flag of Mani Peninsula\n",
      "Loading and filtering chunks for target table...\n",
      "Found 143 chunks that match table ID 'List of chemical element name etymologies_41934D70E87D313A' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "Empty DataFrame\n",
      "Columns: [Description, Language of origin, Original word, Symbol origin]\n",
      "Index: []\n",
      "Loading and filtering chunks for target table...\n",
      "Found 16 chunks that match table ID 'Olympian Gods (DC Comics)_82CC97B123F8ACC0' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                         [hide] v t e Wonder Woman.1\n",
      "0  Wonder Women Diana Prince Orana Artemis of Ban...\n",
      "1  Antiope Aphrodite Artemis Artemis of Bana-Migh...\n",
      "2  Amazons of Themyscira Amazons of Bana-Mighdall...\n",
      "Loading and filtering chunks for target table...\n",
      "Found 23 chunks that match table ID 'The Magicians (U.S. TV series)_C1761A0BEB4A7B16' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                                         Directed by  \\\n",
      "0  Quentin flees the wellspring seeking help and ...   \n",
      "1                                         John Scott   \n",
      "2                                      Joshua Butler   \n",
      "3  Ember wants to destroy Fillory because it got ...   \n",
      "\n",
      "                                       No. in season  \\\n",
      "0  Quentin flees the wellspring seeking help and ...   \n",
      "1                                                  3   \n",
      "2                                                 10   \n",
      "3  Ember wants to destroy Fillory because it got ...   \n",
      "\n",
      "                                         No. overall  \\\n",
      "0  Quentin flees the wellspring seeking help and ...   \n",
      "1                                                 16   \n",
      "2                                                 23   \n",
      "3  Ember wants to destroy Fillory because it got ...   \n",
      "\n",
      "                                          Written by  \n",
      "0  Quentin flees the wellspring seeking help and ...  \n",
      "1                                 Henry Alonso Myers  \n",
      "2                   Noga Landau & Henry Alonso Myers  \n",
      "3  Ember wants to destroy Fillory because it got ...  \n",
      "Loading and filtering chunks for target table...\n",
      "Found 16 chunks that match table ID 'Wonder Woman (TV series)_C16AF4AAF0089316' out of 2881668 total chunks\n",
      "--------------------------------------------------------------------------------\n",
      "final DF\n",
      "                               showvteWonder Woman.1\n",
      "0  Antiope Aphrodite Artemis Artemis of Bana-Migh...\n",
      "1  Amazons of Themyscira Amazons of Bana-Mighdall...\n",
      "Execution time: 131.0694 seconds\n"
     ]
    }
   ],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Determine which table ID to use\n",
    "    query_csv = \"query6_TopTables.csv\"\n",
    "    query_csv_df = pd.read_csv(query_csv)\n",
    "    question = query_csv_df['query'][0]\n",
    "    print(question)\n",
    "    table_list = query_csv_df['top tables'].tolist()\n",
    "    target_table_id = query_csv_df['target table'][0]\n",
    "    goal_answer = query_csv_df['target answer'][0]\n",
    "\n",
    "    # all_entries = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(\"all_pruned_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for table_id in table_list[:10]:\n",
    "    \n",
    "            pruned_df = prune_table(table_id, question)\n",
    "    \n",
    "            if not pruned_df.empty:\n",
    "                entry = dataframe_to_json_entry(pruned_df,table_id)\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
