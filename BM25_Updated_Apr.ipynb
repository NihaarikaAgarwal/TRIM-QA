{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9818c891-84f7-4bdf-bda8-fa52b01beba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: rank_bm25 in ./.local/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from rank_bm25) (2.2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7edeed5-577d-425c-91e2-11d59fbd9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sjain300/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sjain300/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved queries from saved_random_queries.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169898it [00:26, 6400.14it/s] \n",
      "Tokenizing Chunks: 100%|██████████| 2881668/2881668 [23:22<00:00, 2054.38chunk/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized chunks to tokenized_chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   0%|          | 0/150 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'when did season 3 of orange is the new black come out' is: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   1%|          | 1/150 [03:09<7:51:25, 189.84s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'when did season 3 of orange is the new black come out' is: 0.00%\n",
      "Recall for query 'what is the order of the stieg larsson books' is: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   1%|▏         | 2/150 [06:03<7:24:09, 180.07s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'what is the order of the stieg larsson books' is: 100.00%\n",
      "Recall for query 'who did andy murray beat in wimbledon finals' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   2%|▏         | 3/150 [08:53<7:10:02, 175.52s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who did andy murray beat in wimbledon finals' is: 0.00%\n",
      "Recall for query 'who starred in the film walk the line' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   3%|▎         | 4/150 [11:49<7:07:55, 175.86s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who starred in the film walk the line' is: 0.00%\n",
      "Recall for query 'where is the great salt lake located in utah' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   3%|▎         | 5/150 [14:47<7:07:13, 176.78s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'where is the great salt lake located in utah' is: 0.00%\n",
      "Recall for query 'who is the chief of the gods according to ancient greek myth' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   4%|▍         | 6/150 [17:49<7:08:26, 178.52s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who is the chief of the gods according to ancient greek myth' is: 0.00%\n",
      "Recall for query 'who is the head of the national security agency' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   5%|▍         | 7/150 [20:46<7:03:36, 177.74s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who is the head of the national security agency' is: 0.00%\n",
      "Recall for query 'who owns the rights to the black panther movie' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   5%|▌         | 8/150 [23:41<6:58:39, 176.90s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who owns the rights to the black panther movie' is: 0.00%\n",
      "Recall for query 'where is the snooker world open currently held' is: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   6%|▌         | 9/150 [26:34<6:53:05, 175.79s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'where is the snooker world open currently held' is: 0.00%\n",
      "Recall for query 'who won the 1961 college football national championship' is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries:   7%|▋         | 10/150 [29:27<6:48:11, 174.94s/query]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for query 'who won the 1961 college football national championship' is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9|: ]+\", \" \", text)  # retain pipes and colons, remove other punctuation\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "################################ Chunking #################################\n",
    "\n",
    "# Row chunking and its metadata\n",
    "def chunk_row(row, row_id, table_name, columns):\n",
    "    row_text = ' | '.join([f\"{columns[i]['text']}: {cell['text']}\" for i, cell in enumerate(row['cells']) if columns[i]['text']])\n",
    "    return {\n",
    "        \"text\": row_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": row_id,\n",
    "            \"chunk_id\": f\"{table_name}_row_{row_id}\",\n",
    "            \"chunk_type\": \"row\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],\n",
    "            \"metadata_text\": f\"table: {table_name}, row: {row_id}, chunk_id: {table_name}_row_{row_id}, chunk_type: row, columns: {', '.join([col['text'] for col in columns if col['text']])}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Column chunk and its metadata\n",
    "def chunk_column(rows, col_id, col_name, table_name):\n",
    "    column_text = ' | '.join([row['cells'][col_id]['text'] for row in rows if row['cells'][col_id]['text']])\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"{col_name if col_name else ''}: {column_text}\",\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"col_id\": col_id,\n",
    "            \"chunk_id\": f\"{table_name}_column_{col_id}\",\n",
    "            \"chunk_type\": \"column\",\n",
    "            \"metadata_text\": f\"table: {table_name}, col: {col_name if col_name else ''}, chunk_id: {table_name}_column_{col_id}, chunk_type: column\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Table chunking with its metadata\n",
    "def chunk_table(rows, table_id, columns):\n",
    "    column_names = \" | \".join([col['text'] for col in columns])\n",
    "    table_text = '\\n'.join([column_names] + [' | '.join([cell['text'] for cell in row['cells']]) for row in rows])\n",
    "\n",
    "    return {\n",
    "        \"text\": table_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_id,\n",
    "            \"chunk_id\": f\"{table_id}_table\",\n",
    "            \"chunk_type\": \"table\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],  # Adding column names\n",
    "            \"metadata_text\": f\"table_name: {table_id}, chunk_id: {table_id}_table, chunk_type: table, columns: {', '.join([col['text'] for col in columns])}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "######################## Processing ##################################\n",
    "\n",
    "# Process jsonl file: chunking\n",
    "def process_jsonl(file_path):\n",
    "\n",
    "    metadata_list = []\n",
    "    chunks = []\n",
    "    chunk_embeddings = []\n",
    "    table_chunks = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line.strip())\n",
    "            table_id = data['tableId']\n",
    "            rows = data['rows']\n",
    "            columns = data['columns']\n",
    "\n",
    "            # Chunking row\n",
    "            for row_id, row in enumerate(rows):\n",
    "                row_chunk = chunk_row(row, row_id, table_id, columns)\n",
    "                chunks.append(row_chunk)\n",
    "                metadata_list.append(row_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking Column\n",
    "            for col_id, col in enumerate(columns):\n",
    "                if col[\"text\"]:\n",
    "                    col_chunk = chunk_column(rows, col_id, col[\"text\"], table_id)\n",
    "                    chunks.append(col_chunk)\n",
    "                    metadata_list.append(col_chunk[\"metadata\"])\n",
    "\n",
    "            # Chunking table\n",
    "            table_chunk = chunk_table(rows, table_id, columns)\n",
    "            chunks.append(table_chunk)\n",
    "            table_chunks.append(table_chunk)\n",
    "\n",
    "    return metadata_list, chunks, table_chunks\n",
    "\n",
    "\n",
    "# Rank Chunks\n",
    "def rank_chunks_with_bm25(tokenized_chunks, query, top_n):\n",
    "    # Using BM25\n",
    "    bm25 = BM25Okapi([chunk['tokenized_text'] for chunk in tokenized_chunks])\n",
    "    scores = bm25.get_scores(query)\n",
    "\n",
    "    # Sort chunks by BM25 score in descending order\n",
    "    ranked_chunks = sorted(zip(scores, tokenized_chunks), reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Get top N chunks\n",
    "    top_ranked_chunks = ranked_chunks[:top_n]\n",
    "    \n",
    "    return top_ranked_chunks\n",
    "\n",
    "# Save the top N chunks to a file\n",
    "def save_top_chunks(top_chunks, output_dir, output_filename):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(top_chunks, f, indent=2)\n",
    "\n",
    "    print(f\"Saved top chunks to {output_path}\")\n",
    "\n",
    "# Calculate recall, rank, and 10%/20% check\n",
    "def calculate_recall(ranked_chunks, correct_table_id, top_n):\n",
    "    rank = None\n",
    "    for idx, (_, chunk) in enumerate(ranked_chunks):\n",
    "        if chunk['table_id'] == correct_table_id:\n",
    "            rank = idx + 1\n",
    "\n",
    "            # Check if it's in the top 10% or 20%\n",
    "            is_in_top_10 = 1 if rank <= top_n * 0.1 else 0\n",
    "            is_in_top_20 = 1 if rank <= top_n * 0.2 else 0\n",
    "            return 1, rank, is_in_top_10, is_in_top_20\n",
    "\n",
    "    return 0, None, 0, 0  # Relevant item not found\n",
    "\n",
    "def save_tokenized_chunks(tokenized_chunks, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for chunk in tokenized_chunks:\n",
    "            json.dump(chunk, f)\n",
    "            f.write('\\n')\n",
    "    print(f\"Saved tokenized chunks to {filepath}\")\n",
    "\n",
    "def load_tokenized_chunks(filepath):\n",
    "    tokenized_chunks = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokenized_chunks.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(tokenized_chunks)} tokenized chunks from {filepath}\")\n",
    "    return tokenized_chunks\n",
    "\n",
    "# Main script\n",
    "def main(tables_file_path, output_dir, top_n_values, saved_queries_path, tokenized_chunks_file=None):\n",
    "    # Check if the random queries already exist\n",
    "    print(f\"Loading saved queries from {saved_queries_path}\")\n",
    "    with open(saved_queries_path, 'r') as f:\n",
    "        selected_queries = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Load or create tokenized chunks\n",
    "    if tokenized_chunks_file and os.path.exists(tokenized_chunks_file):\n",
    "        tokenized_chunks = load_tokenized_chunks(tokenized_chunks_file)\n",
    "    else:\n",
    "        metadata, chunks, table_chunks = process_jsonl(tables_file_path)\n",
    "        table_chunks = sorted(table_chunks, key=lambda x: x[\"metadata\"][\"table_name\"])\n",
    "    \n",
    "        tokenized_chunks = []\n",
    "        for i, chunk in enumerate(tqdm(table_chunks, desc=\"Tokenizing Chunks\", unit=\"chunk\")):\n",
    "            table_id = chunk['metadata']['table_name']\n",
    "            tokenized_text = tokenize(chunk['text'] + str(chunk['metadata']))\n",
    "            tokenized_chunks.append({\n",
    "                \"table_id\": table_id,\n",
    "                \"tokenized_text\": tokenized_text,\n",
    "            })\n",
    "        if tokenized_chunks_file:\n",
    "            save_tokenized_chunks(tokenized_chunks, tokenized_chunks_file)\n",
    "\n",
    "    total_recall = {top_n: 0 for top_n in top_n_values}\n",
    "    total_queries = 0\n",
    "    total_top_10 = {top_n: 0 for top_n in top_n_values}\n",
    "    total_top_20 = {top_n: 0 for top_n in top_n_values}\n",
    "    results = {top_n: [] for top_n in top_n_values}\n",
    "\n",
    "    # Process selected random queries\n",
    "    for query_data in tqdm(selected_queries, desc=\"Processing Queries\", unit=\"query\"):\n",
    "        query = query_data['questions'][0]['originalText']\n",
    "        correct_table_id = query_data['table']['tableId']\n",
    "\n",
    "        tokenized_query = tokenize(query)\n",
    "\n",
    "        for top_n in top_n_values:\n",
    "            ranked_chunks = rank_chunks_with_bm25(tokenized_chunks, tokenized_query, top_n)\n",
    "\n",
    "            recall, rank, is_in_top_10, is_in_top_20 = calculate_recall(ranked_chunks, correct_table_id, top_n)\n",
    "            print(f\"Recall for query '{query}' is: {recall * 100:.2f}%\")\n",
    "\n",
    "            total_recall[top_n] += recall\n",
    "            total_top_10[top_n] += is_in_top_10\n",
    "            total_top_20[top_n] += is_in_top_20\n",
    "            total_queries += 1\n",
    "\n",
    "            results[top_n].append({\n",
    "                \"Recall\": recall * 100,  # Recall as percentage\n",
    "                \"Rank\": rank if rank is not None else \"Not found\",\n",
    "                \"Ans table in top 10%\": is_in_top_10 * 100,\n",
    "                \"Ans table in top 20%\": is_in_top_20 * 100\n",
    "            })\n",
    "\n",
    "    # Calculate and print overall scores\n",
    "    for top_n in top_n_values:\n",
    "        recall_percentage = (total_recall[top_n] / total_queries) * 100 if total_queries > 0 else 0\n",
    "        total_top_10_percentage = (total_top_10[top_n] / total_queries) * 100 if total_queries > 0 else 0\n",
    "        total_top_20_percentage = (total_top_20[top_n] / total_queries) * 100 if total_queries > 0 else 0\n",
    "        print(f\"Overall Recall (Top {top_n} chunks): {recall_percentage:.2f}%, Top 10%: {total_top_10_percentage:.2f}%, Top 20%: {total_top_20_percentage:.2f}%\")\n",
    "\n",
    "        # Save results to CSV for the current top_n\n",
    "        csv_filename = f\"query_results_top_{top_n}.csv\"\n",
    "        csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "        with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = [\"Recall\", \"Rank\", \"Ans table in top 10%\", \"Ans table in top 20%\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for result in results[top_n]:\n",
    "                writer.writerow(result)\n",
    "\n",
    "        print(f\"Results saved to {csv_filepath}\")\n",
    "\n",
    "        # Save top ranked chunks for the current top_n\n",
    "        save_top_chunks(results[top_n], output_dir, f\"top_chunks_{top_n}.json\")\n",
    "\n",
    "# Execution parameters\n",
    "tables_file_path = \"tables.jsonl\"\n",
    "output_dir = \"\"\n",
    "tokenized_chunks_file = \"tokenized_chunks.jsonl\"\n",
    "saved_queries_path = \"saved_random_queries.jsonl\"\n",
    "top_n = [78, 10]\n",
    "\n",
    "main(tables_file_path, output_dir, top_n, saved_queries_path, tokenized_chunks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c6a85-962d-4422-8a0b-018967f1ebad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
