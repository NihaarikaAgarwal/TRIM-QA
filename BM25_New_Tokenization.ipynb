{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9818c891-84f7-4bdf-bda8-fa52b01beba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: rank_bm25 in ./.local/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: tiktoken in ./.local/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from rank_bm25) (2.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.local/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /packages/apps/jupyter/2025-03-24/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm rank_bm25 tiktoken nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7edeed5-577d-425c-91e2-11d59fbd9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sjain300/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved queries from saved_random_queries.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169898it [00:12, 13619.97it/s]\n",
      "Tokenizing Chunks: 100%|██████████| 850707/850707 [04:13<00:00, 3349.58chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized chunks to tokenized_chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 5000 Queries: 100%|██████████| 150/150 [01:10<00:00,  2.12query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 5000): 97.33%, Top 10%: 85.33%, Top 20%: 89.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 2500 Queries: 100%|██████████| 150/150 [01:05<00:00,  2.29query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 2500): 94.00%, Top 10%: 82.67%, Top 20%: 85.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 1250 Queries: 100%|██████████| 150/150 [01:04<00:00,  2.34query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 1250): 92.00%, Top 10%: 75.33%, Top 20%: 82.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 625 Queries: 100%|██████████| 150/150 [01:12<00:00,  2.06query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 625): 85.33%, Top 10%: 67.33%, Top 20%: 75.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 312 Queries: 100%|██████████| 150/150 [01:29<00:00,  1.68query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 312): 84.00%, Top 10%: 61.33%, Top 20%: 67.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 156 Queries: 100%|██████████| 150/150 [01:38<00:00,  1.52query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 156): 76.00%, Top 10%: 52.67%, Top 20%: 61.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 78 Queries: 100%|██████████| 150/150 [01:12<00:00,  2.08query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 78): 69.33%, Top 10%: 44.00%, Top 20%: 52.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 39 Queries: 100%|██████████| 150/150 [01:09<00:00,  2.17query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 39): 64.00%, Top 10%: 40.00%, Top 20%: 44.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 18 Queries: 100%|██████████| 150/150 [01:08<00:00,  2.19query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 18): 56.67%, Top 10%: 27.33%, Top 20%: 40.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Top 10 Queries: 100%|██████████| 150/150 [01:08<00:00,  2.19query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Recall (Top 10): 46.67%, Top 10%: 27.33%, Top 20%: 38.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import tiktoken\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation but keep pipes/colons\n",
    "    text = re.sub(r\"[^a-z0-9|: ]+\", \" \", text)\n",
    "\n",
    "    # Tokenize using tiktoken\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.decode(enc.encode(text)).split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "################################ Chunking #################################\n",
    "\n",
    "def chunk_rows_in_groups(rows, table_name, columns, group_size=3):\n",
    "    grouped_chunks = []\n",
    "    for i in range(0, len(rows), group_size):\n",
    "        subset = rows[i:i + group_size]\n",
    "        text = ' | '.join([\n",
    "            f\"{columns[j]['text']}: {row['cells'][j]['text']}\"\n",
    "            for row in subset\n",
    "            for j in range(len(columns))\n",
    "            if columns[j]['text']\n",
    "        ])\n",
    "        chunk = {\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"chunk_id\": f\"{table_name}_group_{i // group_size}\",\n",
    "                \"table_name\": table_name,\n",
    "                \"chunk_type\": \"row_group\",\n",
    "                \"metadata_text\": f\"grouped rows from {table_name} starting at row {i}\"\n",
    "            }\n",
    "        }\n",
    "        grouped_chunks.append(chunk)\n",
    "    return grouped_chunks\n",
    "\n",
    "def chunk_column(rows, col_id, col_name, table_name):\n",
    "    column_text = ' | '.join([row['cells'][col_id]['text'] for row in rows if row['cells'][col_id]['text']])\n",
    "    return {\n",
    "        \"text\": f\"{col_name if col_name else ''}: {column_text}\",\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_name,\n",
    "            \"col_id\": col_id,\n",
    "            \"chunk_id\": f\"{table_name}_column_{col_id}\",\n",
    "            \"chunk_type\": \"column\",\n",
    "            \"metadata_text\": f\"table: {table_name}, col: {col_name if col_name else ''}, chunk_id: {table_name}_column_{col_id}, chunk_type: column\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def chunk_table(rows, table_id, columns):\n",
    "    column_names = \" | \".join([col['text'] for col in columns])\n",
    "    table_text = '\\n'.join([column_names] + [' | '.join([cell['text'] for cell in row['cells']]) for row in rows])\n",
    "    return {\n",
    "        \"text\": table_text,\n",
    "        \"metadata\": {\n",
    "            \"table_name\": table_id,\n",
    "            \"chunk_id\": f\"{table_id}_table\",\n",
    "            \"chunk_type\": \"table\",\n",
    "            \"columns\": [col[\"text\"] for col in columns],\n",
    "            \"metadata_text\": f\"table_name: {table_id}, chunk_id: {table_id}_table, chunk_type: table, columns: {', '.join([col['text'] for col in columns])}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "######################## Processing ##################################\n",
    "\n",
    "def process_jsonl(file_path):\n",
    "    metadata_list, chunks, table_chunks = [], [], []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line.strip())\n",
    "            table_id = data['tableId']\n",
    "            rows = data['rows']\n",
    "            columns = data['columns']\n",
    "\n",
    "            grouped_row_chunks = chunk_rows_in_groups(rows, table_id, columns, group_size=3)\n",
    "            for chunk in grouped_row_chunks:\n",
    "                chunks.append(chunk)\n",
    "                metadata_list.append(chunk[\"metadata\"])\n",
    "\n",
    "            # for col_id, col in enumerate(columns):\n",
    "            #     if col[\"text\"]:\n",
    "            #         col_chunk = chunk_column(rows, col_id, col[\"text\"], table_id)\n",
    "            #         chunks.append(col_chunk)\n",
    "            #         metadata_list.append(col_chunk[\"metadata\"])\n",
    "\n",
    "            table_chunk = chunk_table(rows, table_id, columns)\n",
    "            chunks.append(table_chunk)\n",
    "            table_chunks.append(table_chunk)\n",
    "\n",
    "    return metadata_list, chunks, table_chunks\n",
    "\n",
    "def rank_chunks_with_bm25(bm25, tokenized_chunks, query, top_n):\n",
    "    scores = bm25.get_scores(query)\n",
    "    ranked_chunks = sorted(zip(scores, tokenized_chunks), reverse=True, key=lambda x: x[0])\n",
    "    return ranked_chunks[:top_n]\n",
    "\n",
    "def save_top_chunks(top_chunks, output_dir, output_filename):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(top_chunks, f, indent=2)\n",
    "    print(f\"Saved top chunks to {output_path}\")\n",
    "\n",
    "def calculate_recall(ranked_chunks, correct_table_id, top_n):\n",
    "    for idx, (_, chunk) in enumerate(ranked_chunks):\n",
    "        if chunk['table_id'] == correct_table_id:\n",
    "            rank = idx + 1\n",
    "            is_in_top_10 = 1 if rank <= top_n * 0.1 else 0\n",
    "            is_in_top_20 = 1 if rank <= top_n * 0.2 else 0\n",
    "            return 1, rank, is_in_top_10, is_in_top_20\n",
    "    return 0, None, 0, 0\n",
    "\n",
    "def save_tokenized_chunks(tokenized_chunks, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for chunk in tokenized_chunks:\n",
    "            json.dump(chunk, f)\n",
    "            f.write('\\n')\n",
    "    print(f\"Saved tokenized chunks to {filepath}\")\n",
    "\n",
    "def load_tokenized_chunks(filepath):\n",
    "    tokenized_chunks = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokenized_chunks.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(tokenized_chunks)} tokenized chunks from {filepath}\")\n",
    "    return tokenized_chunks\n",
    "\n",
    "############################## MAIN #####################################\n",
    "\n",
    "def main(tables_file_path, file_paths, output_dir, query_output_dir, top_n_values, queries_count, saved_queries_path, tokenized_chunks_file=None):\n",
    "    # Check if the random queries already exist\n",
    "    if os.path.exists(saved_queries_path):\n",
    "        print(f\"Loading saved queries from {saved_queries_path}\")\n",
    "        with open(saved_queries_path, 'r') as f:\n",
    "            selected_queries = [json.loads(line) for line in f][500-queries_count:]\n",
    "    else:\n",
    "        # Combine all queries from the three files\n",
    "        all_queries = []\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line.strip())\n",
    "                    all_queries.append(data)\n",
    "        \n",
    "        # Shuffle queries and select the random queries_count\n",
    "        random.shuffle(all_queries)\n",
    "        selected_queries = all_queries[:queries_count]\n",
    "        \n",
    "        # Save the random queries to a JSONL file for future use\n",
    "        print(f\"Saving random queries to {saved_queries_path}\")\n",
    "        with open(saved_queries_path, 'w') as f:\n",
    "            for query in selected_queries:\n",
    "                json.dump(query, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    if tokenized_chunks_file and os.path.exists(tokenized_chunks_file):\n",
    "        tokenized_chunks = load_tokenized_chunks(tokenized_chunks_file)\n",
    "    else:\n",
    "        metadata, chunks, table_chunks = process_jsonl(tables_file_path)\n",
    "        chunks = sorted(chunks, key=lambda x: x[\"metadata\"][\"table_name\"])\n",
    "\n",
    "        tokenized_chunks = []\n",
    "        for chunk in tqdm(chunks, desc=\"Tokenizing Chunks\", unit=\"chunk\"):\n",
    "            table_id = chunk['metadata']['table_name']\n",
    "            tokenized_text = tokenize(chunk['text'] + str(chunk['metadata']))\n",
    "            tokenized_chunks.append({\n",
    "                \"table_id\": table_id,\n",
    "                \"tokenized_text\": tokenized_text,\n",
    "            })\n",
    "        if tokenized_chunks_file:\n",
    "            save_tokenized_chunks(tokenized_chunks, tokenized_chunks_file)\n",
    "\n",
    "    bm25 = BM25Okapi([chunk['tokenized_text'] for chunk in tokenized_chunks], k1=1.5, b=0.75)\n",
    "    total_recall = {top_n: 0 for top_n in top_n_values}\n",
    "    total_top_10 = {top_n: 0 for top_n in top_n_values}\n",
    "    total_top_20 = {top_n: 0 for top_n in top_n_values}\n",
    "    results = {top_n: [] for top_n in top_n_values}\n",
    "\n",
    "    for top_n in top_n_values:\n",
    "        queries = [(tokenize(q['questions'][0]['originalText']), q['questions'][0]['originalText'], q['table']['tableId'], q['questions'][0]['answer']['answerTexts'][0]) for q in selected_queries]\n",
    "        scores = [bm25.get_scores(query) for query, _, _, _ in queries]\n",
    "        \n",
    "        top_chunks_output_path = os.path.join(output_dir, f\"top_chunks_top_{top_n}.jsonl\")\n",
    "        with open(top_chunks_output_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "            # Create a folder for this top_n value\n",
    "            query_output_dir = os.path.join(output_dir, f\"top_chunks_top_{top_n}\")\n",
    "            os.makedirs(query_output_dir, exist_ok=True)\n",
    "            \n",
    "            for i, (query, query_text, correct_table_id, answer) in enumerate(tqdm(queries, desc=f\"Processing Top {top_n} Queries\", unit=\"query\")):                \n",
    "                ranked_chunks = sorted(zip(scores[i], tokenized_chunks), reverse=True, key=lambda x: x[0])[:top_n]\n",
    "                recall, rank, is_in_top_10, is_in_top_20 = calculate_recall(ranked_chunks, correct_table_id, top_n)\n",
    "\n",
    "                rows = []\n",
    "\n",
    "                for score, chunk in ranked_chunks:\n",
    "                    rows.append({\n",
    "                        \"query\": query_text,\n",
    "                        \"top tables\": chunk[\"table_id\"],\n",
    "                        \"target table\": correct_table_id,\n",
    "                        \"target answer\": answer,\n",
    "                        \"score\": float(score)\n",
    "                    })\n",
    "    \n",
    "                total_recall[top_n] += recall\n",
    "                total_top_10[top_n] += is_in_top_10\n",
    "                total_top_20[top_n] += is_in_top_20\n",
    "    \n",
    "                results[top_n].append({\n",
    "                    \"Recall\": recall * 100,\n",
    "                    \"Rank\": rank if rank is not None else \"Not found\",\n",
    "                    \"Ans table in top 10%\": is_in_top_10 * 100,\n",
    "                    \"Ans table in top 20%\": is_in_top_20 * 100\n",
    "                })\n",
    "\n",
    "                # Save each query result as an individual CSV file\n",
    "                csv_file_path = os.path.join(query_output_dir, f\"query_{i}_top_{top_n}.csv\")\n",
    "                with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"top tables\", \"target table\", \"target answer\", \"score\"])\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(rows)\n",
    "\n",
    "        recall_percentage = (total_recall[top_n] / queries_count) * 100\n",
    "        top_10_percent = (total_top_10[top_n] / queries_count) * 100\n",
    "        top_20_percent = (total_top_20[top_n] / queries_count) * 100\n",
    "\n",
    "        print(f\"Overall Recall (Top {top_n}): {recall_percentage:.2f}%, Top 10%: {top_10_percent:.2f}%, Top 20%: {top_20_percent:.2f}%\")\n",
    "        csv_filename = f\"query_results_top_{top_n}.csv\"\n",
    "        with open(os.path.join(output_dir, csv_filename), 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"Recall\", \"Rank\", \"Ans table in top 10%\", \"Ans table in top 20%\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results[top_n])\n",
    "\n",
    "######################## Execution Parameters ########################\n",
    "\n",
    "# Execution parameters\n",
    "tables_file_path = \"tables.jsonl\"\n",
    "output_dir = \"result\"\n",
    "query_output_dir = \"query_results\"\n",
    "tokenized_chunks_file = \"tokenized_chunks.jsonl\"\n",
    "saved_queries_path = \"saved_random_queries.jsonl\"\n",
    "file_paths = [\"dev.jsonl\", \"train.jsonl\", \"test.jsonl\"]\n",
    "top_n = [5000, 2500, 1250, 625, 312, 156, 78, 39, 18, 10]\n",
    "queries_count = 150\n",
    "\n",
    "main(tables_file_path, file_paths, output_dir, query_output_dir, top_n, queries_count, saved_queries_path, tokenized_chunks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c6a85-962d-4422-8a0b-018967f1ebad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
